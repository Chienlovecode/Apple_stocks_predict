{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huyhuy382003/Apple_stocks_predict/blob/main/Huy_Apple_Stock_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install Java, Spark, Hadoop\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-headless -qq\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "\n",
        "# 2) Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "# 3) Install Python packages\n",
        "!pip install -q pyspark yfinance findspark"
      ],
      "metadata": {
        "id": "f_8y0y32423U",
        "outputId": "8ce267d8-0fdd-48b0-fff3-bcac31d77567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "E: Unable to locate package openjdk-8-headless\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ─ Install Java and Spark ──────────────────────────────────────\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq\n",
        "\n",
        "# Download and unpack Spark\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz -C /usr/local\n",
        "!mv /usr/local/spark-3.3.2-bin-hadoop3 /usr/local/spark\n",
        "\n",
        "# ─ Set environment vars ────────────────────────────────────────\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\""
      ],
      "metadata": {
        "id": "wp3RAJ3i5JaV",
        "outputId": "2ec8d4ed-d484-4222-97df-1fe10e43184c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u452-ga~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u452-ga~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# (re-export in case you restarted runtime)\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()   # no args needed now that SPARK_HOME is set\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"AAPL_Prediction_Colab\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"✅ SparkSession started:\", spark)"
      ],
      "metadata": {
        "id": "2jhrdF1e59ub",
        "outputId": "94f0341c-9093-4e2b-a772-2600f7f267f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SparkSession started: <pyspark.sql.session.SparkSession object at 0x7f1f239429d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Download Apple data in pandas\n",
        "import yfinance as yf\n",
        "pdf = yf.download(\"AAPL\",\n",
        "                  start=\"2018-01-01\",\n",
        "                  progress=False,\n",
        "                  group_by=\"column\",\n",
        "                  auto_adjust=False).reset_index()\n",
        "# Make sure columns are flat and only the ones we need:\n",
        "import pandas as pd\n",
        "if isinstance(pdf.columns, pd.MultiIndex):\n",
        "    pdf.columns = pdf.columns.get_level_values(0)\n",
        "pdf = pdf[[\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]]\n",
        "\n",
        "# 2) Write it out to a local CSV\n",
        "csv_path = \"/content/aapl.csv\"\n",
        "pdf.to_csv(csv_path, index=False)\n",
        "\n",
        "# 3) Let Spark read the CSV (no Python‐object pickling at all!)\n",
        "from pyspark.sql.functions import to_date\n",
        "df = (spark.read\n",
        "           .option(\"header\",\"true\")\n",
        "           .option(\"inferSchema\",\"true\")\n",
        "           .csv(csv_path)\n",
        "           .withColumn(\"Date\", to_date(\"Date\",\"yyyy-MM-dd\"))\n",
        "           .orderBy(\"Date\"))\n",
        "\n",
        "# 4) Now continue with your feature‐engineering + modeling\n",
        "from pyspark.sql.functions import lag, avg\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "w = Window.orderBy(\"Date\")\n",
        "df = (df\n",
        "      .withColumn(\"PrevClose\", lag(\"Close\",1).over(w))\n",
        "      .withColumn(\"MA5\",      avg(\"Close\").over(w.rowsBetween(-5,-1)))\n",
        "      .withColumn(\"MA10\",     avg(\"Close\").over(w.rowsBetween(-10,-1)))\n",
        "      .na.drop())\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Open\",\"High\",\"Low\",\"Volume\",\"PrevClose\",\"MA5\",\"MA10\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "train_df, test_df = df.randomSplit([0.8,0.2], seed=42)\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    labelCol=\"Close\", featuresCol=\"features\",\n",
        "    numTrees=100, maxDepth=5, seed=42\n",
        ")\n",
        "model = Pipeline(stages=[assembler, rf]).fit(train_df)\n",
        "\n",
        "preds = model.transform(test_df)\n",
        "rmse = RegressionEvaluator(\n",
        "    labelCol=\"Close\", predictionCol=\"prediction\",\n",
        "    metricName=\"rmse\"\n",
        ").evaluate(preds)\n",
        "print(f\"AAPL Test RMSE: {rmse:.4f}\")\n",
        "\n",
        "preds.select(\"Date\",\"Close\",\"prediction\") \\\n",
        "     .orderBy(\"Date\") \\\n",
        "     .show(10,truncate=False)\n"
      ],
      "metadata": {
        "id": "XaIOBoCe6aEH",
        "outputId": "c1d22dcd-3eec-4818-9385-607968ebc422",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAPL Test RMSE: 3.1447\n",
            "+----------+------------------+------------------+\n",
            "|Date      |Close             |prediction        |\n",
            "+----------+------------------+------------------+\n",
            "|2018-01-05|43.75             |43.70016875504387 |\n",
            "|2018-01-11|43.81999969482422 |43.731604847009194|\n",
            "|2018-01-16|44.04750061035156 |43.987712890172396|\n",
            "|2018-01-23|44.2599983215332  |43.987712890172396|\n",
            "|2018-01-31|41.85749816894531 |40.10765680971549 |\n",
            "|2018-02-06|40.75749969482422 |40.016806264201115|\n",
            "|2018-02-14|41.842498779296875|40.016806264201115|\n",
            "|2018-02-23|43.875            |43.9436841741815  |\n",
            "|2018-03-09|44.994998931884766|43.987712890172396|\n",
            "|2018-03-12|45.43000030517578 |43.987712890172396|\n",
            "+----------+------------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# RMSE (you already have)\n",
        "rmse_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"Close\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
        ")\n",
        "rmse = rmse_evaluator.evaluate(preds)\n",
        "\n",
        "# MAE\n",
        "mae_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"Close\", predictionCol=\"prediction\", metricName=\"mae\"\n",
        ")\n",
        "mae = mae_evaluator.evaluate(preds)\n",
        "\n",
        "# R²\n",
        "r2_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"Close\", predictionCol=\"prediction\", metricName=\"r2\"\n",
        ")\n",
        "r2 = r2_evaluator.evaluate(preds)\n",
        "\n",
        "print(f\"RMSE = {rmse:.4f}, MAE = {mae:.4f}, R² = {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "mvxXBrwS7o7z",
        "outputId": "abe766de-d866-4198-d21e-3fc9984cbfcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE = 3.1447, MAE = 2.0571, R² = 0.9973\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}